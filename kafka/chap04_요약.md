# 요약

## 📌 리더와 팔로워의 개념
- 하나의 토픽 파티션에는 리더와 팔로워가 있음.
- 리더는 실제로 읽기/쓰기 연산을 처리하는 주체이며 팔로워는 리더의 데이터를 복제함.
- 클러스터 장애 발생 시 ISR(In-Sync Replica)에 속한 팔로워 중 하나가 새로운 리더로 승격됨.

## 📌 복제 유지와 커밋
- 리더와 팔로워는 ISR 그룹에 속해 안정성을 유지.
- ISR에 포함되지 못한 팔로워는 리더 자격을 상실.
- ISR의 동기화 상태를 통해 데이터 일관성과 무결성을 보장.
- 커밋(high water mark)은 모든 복제가 끝난 메시지까지의 위치를 나타냄.
- 컨슈머는 커밋된 메시지만 읽을 수 있음 → 메시지 불일치 방지.

## 📌 리더 선출 전/후 동작
- 장애로 리더가 교체되면 일부 팔로워가 리더로 승격됨.
- 리더 선출 이전에 읽은 메시지가 새 리더와 일치하지 않을 수 있음.
- 이를 통해 리더 장애 시 데이터 불일치 문제와 컨슈머 메시지 일관성 문제를 설명.

## 📌 단계별 리더-팔로워 리플리케이션 동작
1. 리더가 메시지를 오프셋에 저장 → 팔로워는 리더에서 메시지를 가져와 저장.
2. 메시지 저장 및 복제 과정을 오프셋과 함께 시각적으로 설명.
3. 리더가 message1 저장, 팔로워는 아직 미복제 상태.
4. 팔로워가 리더에서 message1 가져와 리플리케이션.
5. 리더가 새로운 message2 저장 → 팔로워는 fetch 요청으로 가져옴.
6. 모든 팔로워가 리더와 동일한 오프셋까지 커밋 완료.
7. ACK 통신 없이도 안정적인 리플리케이션 구조를 구현하여 리더 부하 최소화.

## 📌 리플리케이션 offset checkpoint
- 리더/팔로워는 replication-offset-checkpoint 파일에 마지막 커밋 위치를 기록.
- 이는 장애 복구 시 데이터를 어디까지 복제했는지 확인하는 역할.

## 📌 리더에포크(Leader Epoch)
### 개념
- 리더에포크는 파티션의 리더가 바뀔 때마다 숫자가 1씩 증가함.
- 장애 복구 과정에서 메시지의 일관성을 유지하고, 리더와 팔로워 간 하이워터마크 충돌을 방지하기 위해 사용됨.
- 리더에포크가 없으면 복구 시 메시지 손실이나 중복 가능성 있음.
  
### ✅ 구체 예시로 이해하기

📌 상황 가정
```
파티션 P0이 있고,

Broker A: 리더 (Leader Epoch = 5)

Broker B: 팔로워

Broker C: 팔로워

리더가 메시지 오프셋 0~100까지 썼다고 합시다.
```

📌 팔로워 B의 상태
```
Broker B는 현재 0~100까지 잘 따라갔어요.

Broker B는 메타데이터로 “나는 Leader Epoch 5에서 오프셋 100까지 따라갔다” 라고 기억합니다.
```

### ✅ 리더 장애 발생
```
1️⃣ Broker A가 장애로 다운됨.
2️⃣ 컨트롤러가 Broker B를 새로운 리더로 승격합니다.

이때 Leader Epoch는 5 → 6으로 바뀜.
```
### ✅ 팔로워 C의 복제 과정
```
팔로워 C는 이전 리더(A)에서 098까지만 복제했고
99, 100은 못 받았어요.

그런데 팔로워 C는 새 리더(B)에게
“나는 Leader Epoch 5에서 오프셋 98까지 복제했어” 라고 말합니다.
```
### ✅ 리더 B의 역할
```
리더 B는 팔로워 C의 마지막 오프셋과 에포크를 보고 이렇게 판단합니다:

“팔로워 C는 Epoch 5 시절 데이터 98까지 있으니까,

내가 가진 Epoch 6의 최신 데이터와 비교해 필요한 부분만 보내줘야겠다.”

즉 98~100까지 데이터만 보내주고, 그 뒤 새로 쓴 것(Epoch 6)은 그대로 유지합니다.
```

### ✅ 왜 Leader Epoch가 중요?
```
리더 에포크가 없다면, 팔로워가 “98까지 받았다”는 것만 보내고
그게 이전 리더에서 받았는지 새 리더에서 받았는지 구분 못합니다.

그럼 리더는 아래 둘 중 어느 쪽인지 헷갈립니다:

같은 Epoch라면 그냥 이어서 보내주면 됨.

다른 Epoch라면 데이터 충돌이 있을 수 있으니 중복된 데이터는 삭제하고 다시 보내야 함.

이 차이를 모르고 데이터를 그냥 붙여서 복제하면 → 중복이나 누락이 생깁니다.
```
### ✅ 진짜 핵심
```
팔로워는 “내가 어디까지 복제했는지”뿐만 아니라
“어느 리더(Leader Epoch)에서 복제했는지”까지 꼭 알아야 한다!

그래야 리더는 정확히 필요한 범위만 팔로워에 다시 보내서
데이터 일관성과 정합성을 지킬 수 있어요.
```
## 📌 컨트롤러(Controller)
- 카프카 클러스터의 브로커 중 하나가 컨트롤러 역할을 담당.
- 컨트롤러는 ISR(복제본 리스트)을 관리하며, 브로커 장애 시 새로운 리더를 선출.
- ISR 리스트 정보는 안정적 저장소에 보관.
- 컨트롤러는 리더 정보 변경 시 주키퍼에 기록하고 다른 브로커에 전파.

## 📌 리더 선출 예시
- 브로커 장애 발생 시 리더 선출 과정이 단계별로 도식화됨
- 브로커 다운 → ISR 변화 감지.
- 컨트롤러가 새로운 리더 후보 선택.
- 주키퍼에 리더 정보 기록.
- 정보가 다른 브로커로 전파됨.
- 제어된 종료(Controlled Shutdown) : SIG_TERM 신호로 브로커 종료 → 컨트롤러가 선출 작업 수행 → 다운타임 최소화.

## 📌 로그 세그먼트(Log Segment)
- 토픽의 메시지(레코드)는 로그 세그먼트 파일에 저장됨.
  - .index: 오프셋과 위치 정보.
  - .log: 실제 메시지 데이터.
  - .timeindex: 메시지 타임스탬프.
- 기본 1GB로 롤링(rolling) 전략 적용 → 크기 초과 시 새 세그먼트 생성.

## 📌 로그 세그먼트 삭제/컴팩션
- log.cleanup.policy 옵션으로 관리.
- 삭제(delete): 세그먼트 보관 주기(retention.ms) 만료 시 삭제.
- 컴팩션(compaction): 키 기반 최신 메시지 보존, 나머지 삭제.
- retention.ms, retention.bytes 등으로 세그먼트 삭제 시점/크기 제어 가능.

## 📌 로그 컴팩션 (Log Compaction)
- 개념: Kafka는 메시지 로그에서 오래된 메시지를 삭제하는 일반적인 retention 삭제와 달리, 컴팩션(compaction)은 메시지 키를 기준으로 가장 최신 상태만 유지하는 정책입니다.
- 대표 예: _consumer_offset 토픽.
- 소비자가 읽은 오프셋 정보는 키-밸류 형태로 저장됨.
- 불필요한 과거 메시지는 삭제하고 마지막 오프셋 정보만 유지해 디스크 효율성 ↑.

## 📌 컴팩션 동작 예시
- 오프셋과 키, 밸류가 순서대로 기록됨.
- 동일한 키에 대해 마지막 메시지만 남기고 이전 값은 삭제됨.

## 📌 로그 컴팩션 장점과 주의점
- 장점: 장애 복구 시 전체 로그를 복원하지 않고 키 기준으로 최신 상태만 복원 → 빠른 재처리.
- 주의: 모든 토픽에 무조건 적용하지 않음.
- 최종 상태만 필요한 워크로드에 적합.
- 활성화 시 디스크 I/O 부하 ↑ → 리소스 모니터링 필수.

## 📌 관련 설정 옵션 (표 4-2)
- 옵션 이름	설명
  - cleanup.policy	compact: 토픽 레벨에서 컴팩션 적용
  - log.cleanup.policy	compact: 브로커 설정 파일 단위 적용
  - log.cleaner.min.compaction.lag.ms	메시지 기록 후 컴팩션까지 최소 지연시간
  - log.cleaner.max.compaction.lag.ms	최대 지연시간 지정
  - log.cleaner.min.cleanable.ratio	디스크 상 dirty 비율이 최소 얼마 이상일 때 컴팩션 실행

